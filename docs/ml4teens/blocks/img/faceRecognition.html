<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ml4teens.blocks.img.faceRecognition API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ml4teens.blocks.img.faceRecognition</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os;
import requests;
import numpy as np;
import PIL;
import cv2;

from transformers import AutoImageProcessor, ViTModel;
import torch;

from ultralytics import YOLO;

from tempfile import NamedTemporaryFile;

from PIL.Image import Image;

#from IPython.display import display as IDisplay;

from ...core import Block;

class FaceRecognition(Block):
      &#34;&#34;&#34;
      Dada una imagen de referencia (slot &#34;reference&#34;), con una o más caras que pueden ser detectadas, aprende a reconocer dichas caras en las sucesivas imágenes que se le entreguen (slot &#34;image&#34;).

      Se le puede entregar al constructor un objeto Image (param &#34;reference&#34;) para tener una referencia desde el principio.

      Emite dos señales:

      1) Un diccionario con las caras reconocidas (signal &#34;recognized&#34;).
      2) La imagen de entrada (slot &#34;image&#34;) con las caras reconocidas (slot/param &#34;reference&#34;).

      El formato del diccionario con las facciones reconocidas es el mismo que el de la identificacioón de caras (FaceBoxing) + la etiqueta de la cara reconocida.
      &#34;&#34;&#34;

      #-------------------------------------------------------------------------
      @staticmethod
      def download(source:str):
          if source.startswith(&#34;http&#34;):
             with requests.get(source, stream=True) as r:
                  r.raise_for_status();
                  with NamedTemporaryFile(delete=False) as f:
                       for chunk in r.iter_content(chunk_size=65536//8):
                           f.write(chunk);
                       fuente = f.name;
                       istemp = True;
          else:
             fuente = source;
             istemp = False;

          return (fuente, istemp);

      #-------------------------------------------------------------------------
      @staticmethod
      def similarity(emb1, emb2):
          assert isinstance(emb1, torch.Tensor);
          assert isinstance(emb2, torch.Tensor);
          emb1 = emb1.numpy().flatten();
          emb2 = emb2.numpy().flatten();
          norm1 = emb1 / np.linalg.norm(emb1);
          norm2 = emb2 / np.linalg.norm(emb2);
          return np.dot(norm1, norm2);

      #-------------------------------------------------------------------------
      @staticmethod
      def normalize(image, landmarks):
          # Definir las coordenadas objetivo para los landmarks en la imagen normalizada
          desired_eye_x = 0.25; # Porcentaje del ancho de la imagen para los ojos
          desired_eye_y = 0.4   # Porcentaje del alto de la imagen para los ojos

          desired_nose_x = 0.5  # Mitad de la imagen para la nariz
          desired_nose_y = 0.6  # Porcentaje del alto de la imagen para la nariz

          desired_mouth_x = 0.5  # Mitad de la imagen para la boca
          desired_mouth_y = 0.85; # Porcentaje del alto de la imagen para la boca
            
          img_h, img_w = image.shape[:2]
          desired_width, desired_height = img_w, img_h
            
          # Coordenadas objetivo basadas en las dimensiones deseadas
          desired_left_eye = (int(desired_width * desired_eye_x), int(desired_height * desired_eye_y))
          desired_right_eye = (int(desired_width * (1 - desired_eye_x)), int(desired_height * desired_eye_y))
          desired_nose = (int(desired_width * desired_nose_x), int(desired_height * desired_nose_y))
          desired_left_mouth = (int(desired_width * (desired_mouth_x - 0.1)), int(desired_height * desired_mouth_y))
          desired_right_mouth = (int(desired_width * (desired_mouth_x + 0.1)), int(desired_height * desired_mouth_y))
            
          # Calcular la matriz de transformación para alinear los landmarks a las coordenadas objetivo
          src_pts = np.float32([landmarks[0], landmarks[1], landmarks[2], landmarks[3], landmarks[4]])
          dst_pts = np.float32([desired_left_eye, desired_right_eye, desired_nose, desired_left_mouth, desired_right_mouth])
           
          # Calcular la matriz de transformación usando todos los 5 puntos para obtener más precisión
          transform_matrix, _ = cv2.findHomography(src_pts, dst_pts)
            
          # Aplicar la transformación para normalizar la subimagen
          normalized_image = cv2.warpPerspective(image, transform_matrix, (desired_width, desired_height))

          resized_image = cv2.resize(normalized_image, (128,128), interpolation=cv2.INTER_AREA);
          return resized_image;
          
      #-------------------------------------------------------------------------
      @staticmethod
      def embedding(processor, model, data, lm):
          data=FaceRecognition.normalize(data, lm);
          #IDisplay(PIL.Image.fromarray(data));
          inputs = processor(data, return_tensors=&#34;pt&#34;);
          with torch.no_grad():
               outputs = model(**inputs);
               return outputs.last_hidden_state[0];
              
      #-------------------------------------------------------------------------
      @staticmethod
      def boxes(model, imagen, minconf=0.0):
          results = model(imagen, stream=False, verbose=False);
          rt=[];
          for r in results:
              boxes=[{ &#34;trust&#34;: float(c), 
                       &#34;xyxy&#34; : [int(_) for _ in k],
                       &#34;xy&#34;:    [[int(_) for _ in l]  for l in p], }
                     for c,k,p in zip(r.boxes.conf, r.boxes.xyxy, r.keypoints.xy) if float(c)&gt;=minconf];
              rt += boxes;
          return rt;             
          
      #-------------------------------------------------------------------------
      def encodings(self, imagen, minconf=0.0):
          rt=[];
          boxes = FaceRecognition.boxes(self._facem, imagen, minconf=minconf);
          for box in boxes:
              conf           = box[&#34;trust&#34;];
              x1, y1, x2, y2 = box[&#34;xyxy&#34; ];
              l1,l2,l3,l4,l5 = box[&#34;xy&#34;   ];
              face = imagen.crop( (x1,y1,x2,y2) );
              data=np.array(face);
              l1=(l1[0]-x1, l1[1]-y1);
              l2=(l2[0]-x1, l2[1]-y1);
              l3=(l3[0]-x1, l3[1]-y1);
              l4=(l4[0]-x1, l4[1]-y1);
              l5=(l5[0]-x1, l5[1]-y1);
              embedding = FaceRecognition.embedding(self._processor, self._model, data, (l1,l2,l3,l4,l5));
              rt.append( ( (x1,y1,x2,y2), face, embedding) );
          return rt;
    
      #-------------------------------------------------------------------------
      # Constructor
      #-------------------------------------------------------------------------
      def __init__(self, **kwargs):
          super().__init__(**kwargs);

          cwd = os.path.dirname(__file__);
          mwd = os.path.join(cwd, &#39;../../models&#39;);
          fwd = os.path.join(cwd, &#39;../../fonts&#39;);
          
          self._font = PIL.ImageFont.truetype(os.path.join(fwd, self.params.fontname or &#34;Roboto-Bold.ttf&#34;), self.params.fontsize or 14);
          
          self._facem = YOLO(os.path.join(mwd, &#34;yolov8n-faces.pt&#34;));          
          self._processor = AutoImageProcessor.from_pretrained(&#34;google/vit-base-patch16-224-in21k&#34;);
          self._model     = ViTModel.from_pretrained(&#34;google/vit-base-patch16-224-in21k&#34;);
        
          self._encodings = None;

          if self.params.reference is not None:

             if   isinstance(self.params.reference,str):
                  try:
                    fuente, istemp = FaceRecognition.download(self.params.reference);
                    imagen = PIL.Image.open(fuente);
                    self._encodings = self.encodings(imagen);
                  finally:
                    if istemp: os.remove(fuente);

             elif isinstance(self.params.reference,Image):
                  imagen = self.params.reference;
                  self._encodings = self.encodings(imagen);

             else:
                  raise &#34;El parámetro &#39;reference&#39; debe ser el path de una imagen o un objeto Image&#34;;

      #-------------------------------------------------------------------------
      # SLOTS
      #-------------------------------------------------------------------------
      @Block.slot(&#34;reference&#34;,{Image,str})
      def slot_reference(self, slot, data:(Image|str)):

          if data is None: return;

          assert isinstance(data,(Image,str));

          if   isinstance(data,str):
               try:
                 fuente, istemp = FaceRecognition.download(data);
                 imagen = PIL.Image.open(fuente);
                 self._encodings = self.encodings(imagen);
                 if self.signal_reference():
                    self.signal_reference(imagen);
               finally:
                 if istemp: os.remove(fuente);

          elif isinstance(data,Image):
               imagen = self.params.reference; #.convert(&#34;RGB&#34;);
               self._encodings = self.encodings(imagen);
               if self.signal_reference():
                  self.signal_reference(imagen);

      #-------------------------------------------------------------------------
      @Block.slot(&#34;image&#34;,{Image,str})
      def slot_image(self, slot, data):

          def softmax(x):
              e_x = np.exp(x - np.max(x));
              return e_x / e_x.sum(axis=0);
              #import torch.nn.functional as F
              #scores = torch.tensor(x)
              #softmax_scores = F.softmax(scores, dim=0)
              #return softmax_scores.numpy();


          if self._encodings is not None:

             if isinstance(data,str):
                try:
                  fuente, istemp = FaceRecognition.download(data);
                  data = PIL.Image.open(fuente);
                finally:
                  if istemp: os.remove(fuente);

             else:
                data = data or self.params.reference;

             if data is None: return;

             imagen = data.copy();

             matches=[]; 
             draw=PIL.ImageDraw.Draw(imagen);
             face_encodings = self.encodings(imagen);
             
             for s_enc in self._encodings:
                  
                 encx=[];
                 encs=[];
                 for t_enc in face_encodings:
                     simm=FaceRecognition.similarity(s_enc[2], t_enc[2]);
                     if simm &lt; 0.45: continue;
                     encs.append( [simm, t_enc] );
                     encx.append(  simm         );
                     
                 if encs:    
                    smax=softmax(encx);
                    for i, enc in enumerate(encs):
                        enc[0]=smax[i];                 
                    best=max(encs, key=lambda e: e[0]);
    
                    x1,y1,x2,y2 = best[1][0];
                    conf        = best[0];
                    draw.rectangle([x1,y1,x2,y2], fill=None, outline=(255,0,0), width=5);
                    texto=f&#34;{round(100*float(conf),1)}%&#34;;
                    alto_texto = (self.params.fontsize or 14);
                    draw.text((x1+5+2, y1+5), texto, fill=self.params.fontcolor or &#34;yellow&#34;, font=self._font);
                    matches.append(True);
                 else:    
                    matches.append(False);

             self.signal_recognized(any(matches));             
             self.signal_image(imagen);

      #-------------------------------------------------------------------------
      # SIGNALS
      #-------------------------------------------------------------------------
      @Block.signal(&#34;reference&#34;,Image)
      def signal_reference(self, data:Image):
          return data;

     #-------------------------------------------------------------------------
      @Block.signal(&#34;recognized&#34;,bool)
      def signal_recognized(self, data:bool):
          return data;

      #-------------------------------------------------------------------------
      @Block.signal(&#34;image&#34;,Image)
      def signal_image(self, data:Image):
          return data;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition"><code class="flex name class">
<span>class <span class="ident">FaceRecognition</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Dada una imagen de referencia (slot "reference"), con una o más caras que pueden ser detectadas, aprende a reconocer dichas caras en las sucesivas imágenes que se le entreguen (slot "image").</p>
<p>Se le puede entregar al constructor un objeto Image (param "reference") para tener una referencia desde el principio.</p>
<p>Emite dos señales:</p>
<p>1) Un diccionario con las caras reconocidas (signal "recognized").
2) La imagen de entrada (slot "image") con las caras reconocidas (slot/param "reference").</p>
<p>El formato del diccionario con las facciones reconocidas es el mismo que el de la identificacioón de caras (FaceBoxing) + la etiqueta de la cara reconocida.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FaceRecognition(Block):
      &#34;&#34;&#34;
      Dada una imagen de referencia (slot &#34;reference&#34;), con una o más caras que pueden ser detectadas, aprende a reconocer dichas caras en las sucesivas imágenes que se le entreguen (slot &#34;image&#34;).

      Se le puede entregar al constructor un objeto Image (param &#34;reference&#34;) para tener una referencia desde el principio.

      Emite dos señales:

      1) Un diccionario con las caras reconocidas (signal &#34;recognized&#34;).
      2) La imagen de entrada (slot &#34;image&#34;) con las caras reconocidas (slot/param &#34;reference&#34;).

      El formato del diccionario con las facciones reconocidas es el mismo que el de la identificacioón de caras (FaceBoxing) + la etiqueta de la cara reconocida.
      &#34;&#34;&#34;

      #-------------------------------------------------------------------------
      @staticmethod
      def download(source:str):
          if source.startswith(&#34;http&#34;):
             with requests.get(source, stream=True) as r:
                  r.raise_for_status();
                  with NamedTemporaryFile(delete=False) as f:
                       for chunk in r.iter_content(chunk_size=65536//8):
                           f.write(chunk);
                       fuente = f.name;
                       istemp = True;
          else:
             fuente = source;
             istemp = False;

          return (fuente, istemp);

      #-------------------------------------------------------------------------
      @staticmethod
      def similarity(emb1, emb2):
          assert isinstance(emb1, torch.Tensor);
          assert isinstance(emb2, torch.Tensor);
          emb1 = emb1.numpy().flatten();
          emb2 = emb2.numpy().flatten();
          norm1 = emb1 / np.linalg.norm(emb1);
          norm2 = emb2 / np.linalg.norm(emb2);
          return np.dot(norm1, norm2);

      #-------------------------------------------------------------------------
      @staticmethod
      def normalize(image, landmarks):
          # Definir las coordenadas objetivo para los landmarks en la imagen normalizada
          desired_eye_x = 0.25; # Porcentaje del ancho de la imagen para los ojos
          desired_eye_y = 0.4   # Porcentaje del alto de la imagen para los ojos

          desired_nose_x = 0.5  # Mitad de la imagen para la nariz
          desired_nose_y = 0.6  # Porcentaje del alto de la imagen para la nariz

          desired_mouth_x = 0.5  # Mitad de la imagen para la boca
          desired_mouth_y = 0.85; # Porcentaje del alto de la imagen para la boca
            
          img_h, img_w = image.shape[:2]
          desired_width, desired_height = img_w, img_h
            
          # Coordenadas objetivo basadas en las dimensiones deseadas
          desired_left_eye = (int(desired_width * desired_eye_x), int(desired_height * desired_eye_y))
          desired_right_eye = (int(desired_width * (1 - desired_eye_x)), int(desired_height * desired_eye_y))
          desired_nose = (int(desired_width * desired_nose_x), int(desired_height * desired_nose_y))
          desired_left_mouth = (int(desired_width * (desired_mouth_x - 0.1)), int(desired_height * desired_mouth_y))
          desired_right_mouth = (int(desired_width * (desired_mouth_x + 0.1)), int(desired_height * desired_mouth_y))
            
          # Calcular la matriz de transformación para alinear los landmarks a las coordenadas objetivo
          src_pts = np.float32([landmarks[0], landmarks[1], landmarks[2], landmarks[3], landmarks[4]])
          dst_pts = np.float32([desired_left_eye, desired_right_eye, desired_nose, desired_left_mouth, desired_right_mouth])
           
          # Calcular la matriz de transformación usando todos los 5 puntos para obtener más precisión
          transform_matrix, _ = cv2.findHomography(src_pts, dst_pts)
            
          # Aplicar la transformación para normalizar la subimagen
          normalized_image = cv2.warpPerspective(image, transform_matrix, (desired_width, desired_height))

          resized_image = cv2.resize(normalized_image, (128,128), interpolation=cv2.INTER_AREA);
          return resized_image;
          
      #-------------------------------------------------------------------------
      @staticmethod
      def embedding(processor, model, data, lm):
          data=FaceRecognition.normalize(data, lm);
          #IDisplay(PIL.Image.fromarray(data));
          inputs = processor(data, return_tensors=&#34;pt&#34;);
          with torch.no_grad():
               outputs = model(**inputs);
               return outputs.last_hidden_state[0];
              
      #-------------------------------------------------------------------------
      @staticmethod
      def boxes(model, imagen, minconf=0.0):
          results = model(imagen, stream=False, verbose=False);
          rt=[];
          for r in results:
              boxes=[{ &#34;trust&#34;: float(c), 
                       &#34;xyxy&#34; : [int(_) for _ in k],
                       &#34;xy&#34;:    [[int(_) for _ in l]  for l in p], }
                     for c,k,p in zip(r.boxes.conf, r.boxes.xyxy, r.keypoints.xy) if float(c)&gt;=minconf];
              rt += boxes;
          return rt;             
          
      #-------------------------------------------------------------------------
      def encodings(self, imagen, minconf=0.0):
          rt=[];
          boxes = FaceRecognition.boxes(self._facem, imagen, minconf=minconf);
          for box in boxes:
              conf           = box[&#34;trust&#34;];
              x1, y1, x2, y2 = box[&#34;xyxy&#34; ];
              l1,l2,l3,l4,l5 = box[&#34;xy&#34;   ];
              face = imagen.crop( (x1,y1,x2,y2) );
              data=np.array(face);
              l1=(l1[0]-x1, l1[1]-y1);
              l2=(l2[0]-x1, l2[1]-y1);
              l3=(l3[0]-x1, l3[1]-y1);
              l4=(l4[0]-x1, l4[1]-y1);
              l5=(l5[0]-x1, l5[1]-y1);
              embedding = FaceRecognition.embedding(self._processor, self._model, data, (l1,l2,l3,l4,l5));
              rt.append( ( (x1,y1,x2,y2), face, embedding) );
          return rt;
    
      #-------------------------------------------------------------------------
      # Constructor
      #-------------------------------------------------------------------------
      def __init__(self, **kwargs):
          super().__init__(**kwargs);

          cwd = os.path.dirname(__file__);
          mwd = os.path.join(cwd, &#39;../../models&#39;);
          fwd = os.path.join(cwd, &#39;../../fonts&#39;);
          
          self._font = PIL.ImageFont.truetype(os.path.join(fwd, self.params.fontname or &#34;Roboto-Bold.ttf&#34;), self.params.fontsize or 14);
          
          self._facem = YOLO(os.path.join(mwd, &#34;yolov8n-faces.pt&#34;));          
          self._processor = AutoImageProcessor.from_pretrained(&#34;google/vit-base-patch16-224-in21k&#34;);
          self._model     = ViTModel.from_pretrained(&#34;google/vit-base-patch16-224-in21k&#34;);
        
          self._encodings = None;

          if self.params.reference is not None:

             if   isinstance(self.params.reference,str):
                  try:
                    fuente, istemp = FaceRecognition.download(self.params.reference);
                    imagen = PIL.Image.open(fuente);
                    self._encodings = self.encodings(imagen);
                  finally:
                    if istemp: os.remove(fuente);

             elif isinstance(self.params.reference,Image):
                  imagen = self.params.reference;
                  self._encodings = self.encodings(imagen);

             else:
                  raise &#34;El parámetro &#39;reference&#39; debe ser el path de una imagen o un objeto Image&#34;;

      #-------------------------------------------------------------------------
      # SLOTS
      #-------------------------------------------------------------------------
      @Block.slot(&#34;reference&#34;,{Image,str})
      def slot_reference(self, slot, data:(Image|str)):

          if data is None: return;

          assert isinstance(data,(Image,str));

          if   isinstance(data,str):
               try:
                 fuente, istemp = FaceRecognition.download(data);
                 imagen = PIL.Image.open(fuente);
                 self._encodings = self.encodings(imagen);
                 if self.signal_reference():
                    self.signal_reference(imagen);
               finally:
                 if istemp: os.remove(fuente);

          elif isinstance(data,Image):
               imagen = self.params.reference; #.convert(&#34;RGB&#34;);
               self._encodings = self.encodings(imagen);
               if self.signal_reference():
                  self.signal_reference(imagen);

      #-------------------------------------------------------------------------
      @Block.slot(&#34;image&#34;,{Image,str})
      def slot_image(self, slot, data):

          def softmax(x):
              e_x = np.exp(x - np.max(x));
              return e_x / e_x.sum(axis=0);
              #import torch.nn.functional as F
              #scores = torch.tensor(x)
              #softmax_scores = F.softmax(scores, dim=0)
              #return softmax_scores.numpy();


          if self._encodings is not None:

             if isinstance(data,str):
                try:
                  fuente, istemp = FaceRecognition.download(data);
                  data = PIL.Image.open(fuente);
                finally:
                  if istemp: os.remove(fuente);

             else:
                data = data or self.params.reference;

             if data is None: return;

             imagen = data.copy();

             matches=[]; 
             draw=PIL.ImageDraw.Draw(imagen);
             face_encodings = self.encodings(imagen);
             
             for s_enc in self._encodings:
                  
                 encx=[];
                 encs=[];
                 for t_enc in face_encodings:
                     simm=FaceRecognition.similarity(s_enc[2], t_enc[2]);
                     if simm &lt; 0.45: continue;
                     encs.append( [simm, t_enc] );
                     encx.append(  simm         );
                     
                 if encs:    
                    smax=softmax(encx);
                    for i, enc in enumerate(encs):
                        enc[0]=smax[i];                 
                    best=max(encs, key=lambda e: e[0]);
    
                    x1,y1,x2,y2 = best[1][0];
                    conf        = best[0];
                    draw.rectangle([x1,y1,x2,y2], fill=None, outline=(255,0,0), width=5);
                    texto=f&#34;{round(100*float(conf),1)}%&#34;;
                    alto_texto = (self.params.fontsize or 14);
                    draw.text((x1+5+2, y1+5), texto, fill=self.params.fontcolor or &#34;yellow&#34;, font=self._font);
                    matches.append(True);
                 else:    
                    matches.append(False);

             self.signal_recognized(any(matches));             
             self.signal_image(imagen);

      #-------------------------------------------------------------------------
      # SIGNALS
      #-------------------------------------------------------------------------
      @Block.signal(&#34;reference&#34;,Image)
      def signal_reference(self, data:Image):
          return data;

     #-------------------------------------------------------------------------
      @Block.signal(&#34;recognized&#34;,bool)
      def signal_recognized(self, data:bool):
          return data;

      #-------------------------------------------------------------------------
      @Block.signal(&#34;image&#34;,Image)
      def signal_image(self, data:Image):
          return data;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ml4teens.core.block.Block" href="../../core/block.html#ml4teens.core.block.Block">Block</a></li>
<li>abc.ABC</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.boxes"><code class="name flex">
<span>def <span class="ident">boxes</span></span>(<span>model, imagen, minconf=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def boxes(model, imagen, minconf=0.0):
    results = model(imagen, stream=False, verbose=False);
    rt=[];
    for r in results:
        boxes=[{ &#34;trust&#34;: float(c), 
                 &#34;xyxy&#34; : [int(_) for _ in k],
                 &#34;xy&#34;:    [[int(_) for _ in l]  for l in p], }
               for c,k,p in zip(r.boxes.conf, r.boxes.xyxy, r.keypoints.xy) if float(c)&gt;=minconf];
        rt += boxes;
    return rt;             </code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>source: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def download(source:str):
    if source.startswith(&#34;http&#34;):
       with requests.get(source, stream=True) as r:
            r.raise_for_status();
            with NamedTemporaryFile(delete=False) as f:
                 for chunk in r.iter_content(chunk_size=65536//8):
                     f.write(chunk);
                 fuente = f.name;
                 istemp = True;
    else:
       fuente = source;
       istemp = False;

    return (fuente, istemp);</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.embedding"><code class="name flex">
<span>def <span class="ident">embedding</span></span>(<span>processor, model, data, lm)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def embedding(processor, model, data, lm):
    data=FaceRecognition.normalize(data, lm);
    #IDisplay(PIL.Image.fromarray(data));
    inputs = processor(data, return_tensors=&#34;pt&#34;);
    with torch.no_grad():
         outputs = model(**inputs);
         return outputs.last_hidden_state[0];</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>image, landmarks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def normalize(image, landmarks):
    # Definir las coordenadas objetivo para los landmarks en la imagen normalizada
    desired_eye_x = 0.25; # Porcentaje del ancho de la imagen para los ojos
    desired_eye_y = 0.4   # Porcentaje del alto de la imagen para los ojos

    desired_nose_x = 0.5  # Mitad de la imagen para la nariz
    desired_nose_y = 0.6  # Porcentaje del alto de la imagen para la nariz

    desired_mouth_x = 0.5  # Mitad de la imagen para la boca
    desired_mouth_y = 0.85; # Porcentaje del alto de la imagen para la boca
      
    img_h, img_w = image.shape[:2]
    desired_width, desired_height = img_w, img_h
      
    # Coordenadas objetivo basadas en las dimensiones deseadas
    desired_left_eye = (int(desired_width * desired_eye_x), int(desired_height * desired_eye_y))
    desired_right_eye = (int(desired_width * (1 - desired_eye_x)), int(desired_height * desired_eye_y))
    desired_nose = (int(desired_width * desired_nose_x), int(desired_height * desired_nose_y))
    desired_left_mouth = (int(desired_width * (desired_mouth_x - 0.1)), int(desired_height * desired_mouth_y))
    desired_right_mouth = (int(desired_width * (desired_mouth_x + 0.1)), int(desired_height * desired_mouth_y))
      
    # Calcular la matriz de transformación para alinear los landmarks a las coordenadas objetivo
    src_pts = np.float32([landmarks[0], landmarks[1], landmarks[2], landmarks[3], landmarks[4]])
    dst_pts = np.float32([desired_left_eye, desired_right_eye, desired_nose, desired_left_mouth, desired_right_mouth])
     
    # Calcular la matriz de transformación usando todos los 5 puntos para obtener más precisión
    transform_matrix, _ = cv2.findHomography(src_pts, dst_pts)
      
    # Aplicar la transformación para normalizar la subimagen
    normalized_image = cv2.warpPerspective(image, transform_matrix, (desired_width, desired_height))

    resized_image = cv2.resize(normalized_image, (128,128), interpolation=cv2.INTER_AREA);
    return resized_image;</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.similarity"><code class="name flex">
<span>def <span class="ident">similarity</span></span>(<span>emb1, emb2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def similarity(emb1, emb2):
    assert isinstance(emb1, torch.Tensor);
    assert isinstance(emb2, torch.Tensor);
    emb1 = emb1.numpy().flatten();
    emb2 = emb2.numpy().flatten();
    norm1 = emb1 / np.linalg.norm(emb1);
    norm2 = emb2 / np.linalg.norm(emb2);
    return np.dot(norm1, norm2);</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.encodings"><code class="name flex">
<span>def <span class="ident">encodings</span></span>(<span>self, imagen, minconf=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encodings(self, imagen, minconf=0.0):
    rt=[];
    boxes = FaceRecognition.boxes(self._facem, imagen, minconf=minconf);
    for box in boxes:
        conf           = box[&#34;trust&#34;];
        x1, y1, x2, y2 = box[&#34;xyxy&#34; ];
        l1,l2,l3,l4,l5 = box[&#34;xy&#34;   ];
        face = imagen.crop( (x1,y1,x2,y2) );
        data=np.array(face);
        l1=(l1[0]-x1, l1[1]-y1);
        l2=(l2[0]-x1, l2[1]-y1);
        l3=(l3[0]-x1, l3[1]-y1);
        l4=(l4[0]-x1, l4[1]-y1);
        l5=(l5[0]-x1, l5[1]-y1);
        embedding = FaceRecognition.embedding(self._processor, self._model, data, (l1,l2,l3,l4,l5));
        rt.append( ( (x1,y1,x2,y2), face, embedding) );
    return rt;</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_image"><code class="name flex">
<span>def <span class="ident">signal_image</span></span>(<span>self, data='86e49fcaeea3fef192ed01c43e4ab29dfd685cd2937d9b11a8d5173dab20b134')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrapper(self, data=_unique_):
    using=self.checkSignalUsage(name);
    if data is _unique_:
       return using;
    else:
       if using:
          data=func(self,data);
          Context.instance.emit(source=self, sname=name, data=data, mods=self._signal_mods);</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_recognized"><code class="name flex">
<span>def <span class="ident">signal_recognized</span></span>(<span>self, data='45665e540803f89c44dbd8da0b36e112c63bde317dcecbba5a29705a9a6d2b4e')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrapper(self, data=_unique_):
    using=self.checkSignalUsage(name);
    if data is _unique_:
       return using;
    else:
       if using:
          data=func(self,data);
          Context.instance.emit(source=self, sname=name, data=data, mods=self._signal_mods);</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_reference"><code class="name flex">
<span>def <span class="ident">signal_reference</span></span>(<span>self, data='76d1d62a0be084a26249d94ba867d78e49afdaaaffb95f82655574223d14fe33')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrapper(self, data=_unique_):
    using=self.checkSignalUsage(name);
    if data is _unique_:
       return using;
    else:
       if using:
          data=func(self,data);
          Context.instance.emit(source=self, sname=name, data=data, mods=self._signal_mods);</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_image"><code class="name flex">
<span>def <span class="ident">slot_image</span></span>(<span>self, _slot, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrapper(self, _slot, data):
    try:
      assert name == _slot;
      debug.print(f&#34;Ejecutando {self._fullClassName}::slot(&#39;{_slot}&#39;,{type(data).__name__})&#39;&#34;);
      assert func is not None;
      assert callable(func);
      
      done=func(self, _slot, data);
      
      if self.params.done is not None:
         if callable(self.params.done):
            done=self.params.done(data);
            self.signal_done(done);
         else:
            done=self.params.done;
            self.signal_done(done);
      else:
         if bool(done) is True: # SUGGEST: enviar el valor de &#39;done&#39; si no es None.
            self.signal_done(f&#34;{self._fullClassName}::{_slot}&#34;);
         
    except Exception as e:
      debug.print(f&#34;{cls}:: Excepción: &#39;{e}&#39;&#34;, exception=e);
      
    finally:
      pass;</code></pre>
</details>
</dd>
<dt id="ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_reference"><code class="name flex">
<span>def <span class="ident">slot_reference</span></span>(<span>self, _slot, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrapper(self, _slot, data):
    try:
      assert name == _slot;
      debug.print(f&#34;Ejecutando {self._fullClassName}::slot(&#39;{_slot}&#39;,{type(data).__name__})&#39;&#34;);
      assert func is not None;
      assert callable(func);
      
      done=func(self, _slot, data);
      
      if self.params.done is not None:
         if callable(self.params.done):
            done=self.params.done(data);
            self.signal_done(done);
         else:
            done=self.params.done;
            self.signal_done(done);
      else:
         if bool(done) is True: # SUGGEST: enviar el valor de &#39;done&#39; si no es None.
            self.signal_done(f&#34;{self._fullClassName}::{_slot}&#34;);
         
    except Exception as e:
      debug.print(f&#34;{cls}:: Excepción: &#39;{e}&#39;&#34;, exception=e);
      
    finally:
      pass;</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ml4teens.core.block.Block" href="../../core/block.html#ml4teens.core.block.Block">Block</a></b></code>:
<ul class="hlist">
<li><code><a title="ml4teens.core.block.Block.checkSignalUsage" href="../../core/block.html#ml4teens.core.block.Block.checkSignalUsage">checkSignalUsage</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ml4teens.blocks.img" href="index.html">ml4teens.blocks.img</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition">FaceRecognition</a></code></h4>
<ul class="two-column">
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.boxes" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.boxes">boxes</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.download" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.download">download</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.embedding" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.embedding">embedding</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.encodings" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.encodings">encodings</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.normalize" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.normalize">normalize</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_image" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_image">signal_image</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_recognized" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_recognized">signal_recognized</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_reference" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.signal_reference">signal_reference</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.similarity" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.similarity">similarity</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_image" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_image">slot_image</a></code></li>
<li><code><a title="ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_reference" href="#ml4teens.blocks.img.faceRecognition.FaceRecognition.slot_reference">slot_reference</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>